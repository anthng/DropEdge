{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "blessed-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "try:\n",
    "    from src.earlystopping import EarlyStopping\n",
    "except:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.realpath('../'))\n",
    "    from src.earlystopping import EarlyStopping\n",
    "    \n",
    "from src.sample import Sampler\n",
    "from src.metric import accuracy, roc_auc_compute_fn\n",
    "# from deepgcn.utils import load_data, accuracy\n",
    "# from deepgcn.models import GCN\n",
    "\n",
    "from src.metric import accuracy\n",
    "from src.utils import load_citation, load_reddit_data\n",
    "from src.models import *\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "opt = parser.parse_args([])\n",
    "\n",
    "opt.debug = True\n",
    "opt.dataset = \"cora\"\n",
    "opt.datapath = \"data/\"\n",
    "parser.add_argument('--dataset', default=\"cora\", help=\"The data set\")\n",
    "\n",
    "opt.seed = 42\n",
    "opt.epochs = 50\n",
    "opt.lr = 0.02\n",
    "opt.weight_decay = 5e-4\n",
    "opt.hidden = 128\n",
    "opt.dropout = 0\n",
    "opt.nhiddenlayer=1\n",
    "opt.early_stopping = 0\n",
    "opt.sampling_percent = 1.0\n",
    "\n",
    "opt.normalization = \"AugNormAdj\"\n",
    "opt.nbaseblocklayer = 6\n",
    "opt.type = \"inceptiongcn\"\n",
    "\n",
    "opt.inputlayer = 'gcn'\n",
    "opt.outputlayer = 'gcn'\n",
    "\n",
    "opt.lradjust = False\n",
    "opt.no_cuda = False\n",
    "opt.mixmode = False\n",
    "opt.aggrmethod = \"default\"\n",
    "opt.withloop = False\n",
    "opt.no_tensorboard = False\n",
    "opt.withbn = False\n",
    "opt.fastmode = \"default\"\n",
    "opt.warm_start = \"\"\n",
    "\n",
    "opt.no_cuda = False\n",
    "opt.task_type = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "accepting-plaintiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(aggrmethod='default', datapath='data/', dataset='cora', debug=True, dropout=0, early_stopping=0, epochs=50, fastmode='default', hidden=128, inputlayer='gcn', lr=0.02, lradjust=False, mixmode=False, nbaseblocklayer=6, nhiddenlayer=1, no_cuda=False, no_tensorboard=False, normalization='AugNormAdj', outputlayer='gcn', sampling_percent=1.0, seed=42, task_type='full', type='inceptiongcn', warm_start='', weight_decay=0.0005, withbn=False, withloop=False)\n",
      "Load full supervised task.\n"
     ]
    }
   ],
   "source": [
    "if opt.debug:\n",
    "    print(opt)\n",
    "# pre setting\n",
    "opt.cuda = not opt.no_cuda and torch.cuda.is_available()\n",
    "opt.mixmode = opt.no_cuda and opt.mixmode and torch.cuda.is_available()\n",
    "if opt.aggrmethod == \"default\":\n",
    "    if opt.type == \"resgcn\":\n",
    "        opt.aggrmethod = \"add\"\n",
    "    else:\n",
    "        opt.aggrmethod = \"concat\"\n",
    "if opt.fastmode and opt.early_stopping > 0:\n",
    "    opt.early_stopping = 0\n",
    "    print(\"In the fast mode, early_stopping is not valid option. Setting early_stopping = 0.\")\n",
    "if opt.type == \"mutigcn\":\n",
    "    print(\"For the multi-layer gcn model, the aggrmethod is fixed to nores and nhiddenlayers = 1.\")\n",
    "    opt.nhiddenlayer = 1\n",
    "    opt.aggrmethod = \"nores\"\n",
    "\n",
    "# random seed setting\n",
    "np.random.seed(opt.seed)\n",
    "torch.manual_seed(opt.seed)\n",
    "if opt.cuda or opt.mixmode:\n",
    "    torch.cuda.manual_seed(opt.seed)\n",
    "\n",
    "# should we need fix random seed here?\n",
    "sampler = Sampler(opt.dataset, opt.datapath, opt.task_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "athletic-nutrition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nclass: 7\tnfeat: 1433\n"
     ]
    }
   ],
   "source": [
    "# get labels and indexes\n",
    "labels, idx_train, idx_val, idx_test = sampler.get_label_and_idxes(opt.cuda)\n",
    "nfeat = sampler.nfeat\n",
    "nclass = sampler.nclass\n",
    "print(\"nclass: %d\\tnfeat: %d\" % (nclass, nfeat))\n",
    "\n",
    "# The model\n",
    "model = GCNModel(nfeat=nfeat,\n",
    "                 nhid=opt.hidden,\n",
    "                 nclass=nclass,\n",
    "                 nhidlayer=opt.nhiddenlayer,\n",
    "                 dropout=opt.dropout,\n",
    "                 baseblock=opt.type,\n",
    "                 inputlayer=opt.inputlayer,\n",
    "                 outputlayer=opt.outputlayer,\n",
    "                 nbaselayer=opt.nbaseblocklayer,\n",
    "                 activation=F.relu,\n",
    "                 withbn=opt.withbn,\n",
    "                 withloop=opt.withloop,\n",
    "                 aggrmethod=opt.aggrmethod,\n",
    "                 mixmode=opt.mixmode)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=opt.lr, weight_decay=opt.weight_decay)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=50, factor=0.618)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 300, 400, 500, 600, 700], gamma=0.5)\n",
    "# convert to cuda\n",
    "if opt.cuda:\n",
    "    model.cuda()\n",
    "\n",
    "# For the mix mode, lables and indexes are in cuda. \n",
    "if opt.cuda or opt.mixmode:\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "if opt.warm_start is not None and opt.warm_start != \"\":\n",
    "    early_stopping = EarlyStopping(fname=opt.warm_start, verbose=False)\n",
    "    print(\"Restore checkpoint from %s\" % (early_stopping.fname))\n",
    "    model.load_state_dict(early_stopping.load_checkpoint())\n",
    "\n",
    "# set early_stopping\n",
    "if opt.early_stopping > 0:\n",
    "    early_stopping = EarlyStopping(patience=opt.early_stopping, verbose=False)\n",
    "    print(\"Model is saving to: %s\" % (early_stopping.fname))\n",
    "\n",
    "if opt.no_tensorboard is False:\n",
    "    tb_writer = SummaryWriter(\n",
    "        comment=f\"-dataset_{opt.dataset}-type_{opt.type}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "overhead-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# define the training function.\n",
    "def train(epoch, train_adj, train_fea, idx_train, val_adj=None, val_fea=None):\n",
    "    if val_adj is None:\n",
    "        val_adj = train_adj\n",
    "        val_fea = train_fea\n",
    "\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_fea, train_adj)\n",
    "    # special for reddit\n",
    "    if sampler.learning_type == \"inductive\":\n",
    "        loss_train = F.nll_loss(output, labels[idx_train])\n",
    "        acc_train = accuracy(output, labels[idx_train])\n",
    "    else:\n",
    "        loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    train_t = time.time() - t\n",
    "    val_t = time.time()\n",
    "    # We can not apply the fastmode for the reddit dataset.\n",
    "    # if sampler.learning_type == \"inductive\" or not opt.fastmode:\n",
    "\n",
    "    if opt.early_stopping > 0 and sampler.dataset != \"reddit\":\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val]).item()\n",
    "        early_stopping(loss_val, model)\n",
    "\n",
    "    if not opt.fastmode:\n",
    "        #    # Evaluate validation set performance separately,\n",
    "        #    # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(val_fea, val_adj)\n",
    "        loss_val = F.nll_loss(output[idx_val], labels[idx_val]).item()\n",
    "        acc_val = accuracy(output[idx_val], labels[idx_val]).item()\n",
    "        if sampler.dataset == \"reddit\":\n",
    "            early_stopping(loss_val, model)\n",
    "    else:\n",
    "        loss_val = 0\n",
    "        acc_val = 0\n",
    "\n",
    "    if opt.lradjust:\n",
    "        scheduler.step()\n",
    "\n",
    "    val_t = time.time() - val_t\n",
    "    return (loss_train.item(), acc_train.item(), loss_val, acc_val, get_lr(optimizer), train_t, val_t)\n",
    "\n",
    "\n",
    "def test(test_adj, test_fea):\n",
    "    model.eval()\n",
    "    output = model(test_fea, test_adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    auc_test = roc_auc_compute_fn(output[idx_test], labels[idx_test])\n",
    "    if opt.debug:\n",
    "        print(\"Test set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"auc= {:.4f}\".format(auc_test),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        print(\"accuracy=%.5f\" % (acc_test.item()))\n",
    "    return (loss_test.item(), acc_test.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "injured-textbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9663 acc_train: 0.2343 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3150s v_time: 0.0000s\n",
      "Epoch: 0002 loss_train: 3.5028 acc_train: 0.2914 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3050s v_time: 0.0000s\n",
      "Epoch: 0003 loss_train: 2.5950 acc_train: 0.1714 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3160s v_time: 0.0000s\n",
      "Epoch: 0004 loss_train: 2.1288 acc_train: 0.2161 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.4230s v_time: 0.0000s\n",
      "Epoch: 0005 loss_train: 1.8107 acc_train: 0.3137 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.4080s v_time: 0.0000s\n",
      "Epoch: 0006 loss_train: 1.9231 acc_train: 0.2914 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3190s v_time: 0.0000s\n",
      "Epoch: 0007 loss_train: 1.7579 acc_train: 0.2939 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0010s t_time: 0.3660s v_time: 0.0000s\n",
      "Epoch: 0008 loss_train: 1.7521 acc_train: 0.3990 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3240s v_time: 0.0000s\n",
      "Epoch: 0009 loss_train: 1.7051 acc_train: 0.4892 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3660s v_time: 0.0000s\n",
      "Epoch: 0010 loss_train: 1.6119 acc_train: 0.6805 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.4680s v_time: 0.0000s\n",
      "Epoch: 0011 loss_train: 1.5095 acc_train: 0.5306 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3490s v_time: 0.0000s\n",
      "Epoch: 0012 loss_train: 1.4232 acc_train: 0.4636 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3650s v_time: 0.0000s\n",
      "Epoch: 0013 loss_train: 1.2694 acc_train: 0.6639 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3590s v_time: 0.0000s\n",
      "Epoch: 0014 loss_train: 1.1368 acc_train: 0.7748 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3170s v_time: 0.0000s\n",
      "Epoch: 0015 loss_train: 0.9840 acc_train: 0.7707 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3660s v_time: 0.0000s\n",
      "Epoch: 0016 loss_train: 0.8344 acc_train: 0.7575 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3260s v_time: 0.0000s\n",
      "Epoch: 0017 loss_train: 0.7191 acc_train: 0.7732 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3230s v_time: 0.0000s\n",
      "Epoch: 0018 loss_train: 0.6220 acc_train: 0.8212 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.6080s v_time: 0.0000s\n",
      "Epoch: 0019 loss_train: 0.5531 acc_train: 0.8576 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3960s v_time: 0.0000s\n",
      "Epoch: 0020 loss_train: 0.5024 acc_train: 0.8568 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3520s v_time: 0.0000s\n",
      "Epoch: 0021 loss_train: 0.4661 acc_train: 0.8667 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3260s v_time: 0.0000s\n",
      "Epoch: 0022 loss_train: 0.4124 acc_train: 0.8940 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3450s v_time: 0.0000s\n",
      "Epoch: 0023 loss_train: 0.3685 acc_train: 0.9007 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3640s v_time: 0.0000s\n",
      "Epoch: 0024 loss_train: 0.3522 acc_train: 0.8982 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3281s v_time: 0.0000s\n",
      "Epoch: 0025 loss_train: 0.3237 acc_train: 0.9056 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3230s v_time: 0.0000s\n",
      "Epoch: 0026 loss_train: 0.3105 acc_train: 0.9131 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3510s v_time: 0.0000s\n",
      "Epoch: 0027 loss_train: 0.2880 acc_train: 0.9106 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3120s v_time: 0.0000s\n",
      "Epoch: 0028 loss_train: 0.2642 acc_train: 0.9222 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3330s v_time: 0.0000s\n",
      "Epoch: 0029 loss_train: 0.2524 acc_train: 0.9247 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3430s v_time: 0.0000s\n",
      "Epoch: 0030 loss_train: 0.2513 acc_train: 0.9172 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3010s v_time: 0.0000s\n",
      "Epoch: 0031 loss_train: 0.2375 acc_train: 0.9346 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3150s v_time: 0.0000s\n",
      "Epoch: 0032 loss_train: 0.2225 acc_train: 0.9346 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.4430s v_time: 0.0000s\n",
      "Epoch: 0033 loss_train: 0.2220 acc_train: 0.9238 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3540s v_time: 0.0000s\n",
      "Epoch: 0034 loss_train: 0.2001 acc_train: 0.9387 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3360s v_time: 0.0000s\n",
      "Epoch: 0035 loss_train: 0.1957 acc_train: 0.9387 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.4410s v_time: 0.0000s\n",
      "Epoch: 0036 loss_train: 0.2052 acc_train: 0.9313 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3120s v_time: 0.0000s\n",
      "Epoch: 0037 loss_train: 0.2036 acc_train: 0.9338 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3410s v_time: 0.0000s\n",
      "Epoch: 0038 loss_train: 0.2316 acc_train: 0.9255 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3050s v_time: 0.0000s\n",
      "Epoch: 0039 loss_train: 0.2901 acc_train: 0.8998 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3010s v_time: 0.0000s\n",
      "Epoch: 0040 loss_train: 0.3237 acc_train: 0.8974 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3250s v_time: 0.0000s\n",
      "Epoch: 0041 loss_train: 0.1873 acc_train: 0.9445 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3210s v_time: 0.0000s\n",
      "Epoch: 0042 loss_train: 0.2895 acc_train: 0.8965 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.2820s v_time: 0.0000s\n",
      "Epoch: 0043 loss_train: 0.2116 acc_train: 0.9321 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3230s v_time: 0.0000s\n",
      "Epoch: 0044 loss_train: 0.2618 acc_train: 0.9147 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3860s v_time: 0.0000s\n",
      "Epoch: 0045 loss_train: 0.1999 acc_train: 0.9396 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3600s v_time: 0.0000s\n",
      "Epoch: 0046 loss_train: 0.1850 acc_train: 0.9387 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.4530s v_time: 0.0000s\n",
      "Epoch: 0047 loss_train: 0.2247 acc_train: 0.9263 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.4240s v_time: 0.0000s\n",
      "Epoch: 0048 loss_train: 0.1714 acc_train: 0.9412 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3290s v_time: 0.0000s\n",
      "Epoch: 0049 loss_train: 0.1812 acc_train: 0.9437 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3690s v_time: 0.0000s\n",
      "Epoch: 0050 loss_train: 0.2051 acc_train: 0.9346 loss_val: 0.0000 acc_val: 0.0000 cur_lr: 0.02000 s_time: 0.0000s t_time: 0.3020s v_time: 0.0000s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 17.8350s\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_train = np.zeros((opt.epochs,))\n",
    "acc_train = np.zeros((opt.epochs,))\n",
    "loss_val = np.zeros((opt.epochs,))\n",
    "acc_val = np.zeros((opt.epochs,))\n",
    "\n",
    "sampling_t = 0\n",
    "\n",
    "for epoch in range(opt.epochs):\n",
    "    input_idx_train = idx_train\n",
    "    sampling_t = time.time()\n",
    "    # no sampling\n",
    "    # randomedge sampling if opt.sampling_percent >= 1.0, it behaves the same as stub_sampler.\n",
    "    (train_adj, train_fea) = sampler.randomedge_sampler(percent=opt.sampling_percent, normalization=opt.normalization,\n",
    "                                                        cuda=opt.cuda)\n",
    "    if opt.mixmode:\n",
    "        train_adj = train_adj.cuda()\n",
    "\n",
    "    sampling_t = time.time() - sampling_t\n",
    "    \n",
    "    # The validation set is controlled by idx_val\n",
    "    # if sampler.learning_type == \"transductive\":\n",
    "    if False:\n",
    "        outputs = train(epoch, train_adj, train_fea, input_idx_train)\n",
    "    else:\n",
    "        (val_adj, val_fea) = sampler.get_test_set(normalization=opt.normalization, cuda=opt.cuda)\n",
    "        if opt.mixmode:\n",
    "            val_adj = val_adj.cuda()\n",
    "        outputs = train(epoch, train_adj, train_fea, input_idx_train, val_adj, val_fea)\n",
    "\n",
    "    if opt.debug and epoch % 1 == 0:\n",
    "        print('Epoch: {:04d}'.format(epoch + 1),\n",
    "              'loss_train: {:.4f}'.format(outputs[0]),\n",
    "              'acc_train: {:.4f}'.format(outputs[1]),\n",
    "              'loss_val: {:.4f}'.format(outputs[2]),\n",
    "              'acc_val: {:.4f}'.format(outputs[3]),\n",
    "              'cur_lr: {:.5f}'.format(outputs[4]),\n",
    "              's_time: {:.4f}s'.format(sampling_t),\n",
    "              't_time: {:.4f}s'.format(outputs[5]),\n",
    "              'v_time: {:.4f}s'.format(outputs[6]))\n",
    "    \n",
    "    if opt.no_tensorboard is False:\n",
    "        tb_writer.add_scalars('Loss', {'train': outputs[0], 'val': outputs[2]}, epoch)\n",
    "        tb_writer.add_scalars('Accuracy', {'train': outputs[1], 'val': outputs[3]}, epoch)\n",
    "        tb_writer.add_scalar('lr', outputs[4], epoch)\n",
    "        tb_writer.add_scalars('Time', {'train': outputs[5], 'val': outputs[6]}, epoch)\n",
    "        \n",
    "\n",
    "    loss_train[epoch], acc_train[epoch], loss_val[epoch], acc_val[epoch] = outputs[0], outputs[1], outputs[2], outputs[\n",
    "        3]\n",
    "\n",
    "    if opt.early_stopping > 0 and early_stopping.early_stop:\n",
    "        print(\"Early stopping.\")\n",
    "        model.load_state_dict(early_stopping.load_checkpoint())\n",
    "        break\n",
    "\n",
    "if opt.early_stopping > 0:\n",
    "    model.load_state_dict(early_stopping.load_checkpoint())\n",
    "\n",
    "if opt.debug:\n",
    "    print(\"Optimization Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "pressed-egypt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.5295 auc= 0.9747 accuracy= 0.8530\n",
      "accuracy=0.85300\n",
      "loss_train\tloss_val\tloss_test\tacc_train\tacc_val\t\tacc_test\n",
      "0.205118\t0.000000\t0.529509\t0.934603\t0.000000\t0.853000\n"
     ]
    }
   ],
   "source": [
    "(test_adj, test_fea) = sampler.get_test_set(normalization=opt.normalization, cuda=opt.cuda)\n",
    "if opt.mixmode:\n",
    "    test_adj = test_adj.cuda()\n",
    "(loss_test, acc_test) = test(test_adj, test_fea)\n",
    "\n",
    "print(\"%s\\t%s\\t%s\\t%s\\t%s\\t\\t%s\" % (\"loss_train\", \"loss_val\", \"loss_test\", \"acc_train\", \"acc_val\", \"acc_test\"))\n",
    "print(\"%.6f\\t%.6f\\t%.6f\\t%.6f\\t%.6f\\t%.6f\" % (loss_train[-1], loss_val[-1], loss_test, acc_train[-1], acc_val[-1], acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-professional",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".envGCN",
   "language": "python",
   "name": ".envgcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
